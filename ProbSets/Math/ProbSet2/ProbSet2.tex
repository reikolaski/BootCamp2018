\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{frame=single,
  language=Python,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{harvard}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{amsmath}
\let\vec\mathbf
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\setlength{\parindent}{0pt}

\begin{document}

\begin{flushleft}
  \textbf{\large{Problem Set \#2}} \\
  Reiko Laski
\end{flushleft}

\textbf{Exercise 1} \\
(i) \textit{Proof:}
\begin{align*}
  \langle \vec{x}, \vec{y} \rangle &= \frac{1}{4}(||\vec{x} + \vec{y}||^2 - ||\vec{x} - \vec{y}||^2) \\
  &= \frac{1}{4} (\langle \vec{x} + \vec{y}, \vec{x} + \vec{y}\rangle - \langle \vec{x} - \vec{y}, \vec{x} - \vec{y}\rangle) \\
  &= \frac{1}{4} (\langle \vec{x}, \vec{x} + \vec{y}\rangle + \langle \vec{y}, \vec{x} + \vec{y}\rangle - \langle \vec{x}, \vec{x} - \vec{y}\rangle + \langle \vec{y}, \vec{x} - \vec{y}\rangle) \\
  &= \frac{1}{4} (\langle \vec{x}, \vec{x}\rangle + \langle \vec{x}, \vec{y}\rangle + \langle \vec{y}, \vec{x}\rangle + \langle \vec{y}, \vec{y}\rangle - \langle \vec{x}, \vec{x}\rangle + \langle \vec{x}, \vec{y}\rangle + \langle \vec{y}, \vec{x}\rangle - \langle \vec{y}, \vec{y}\rangle) \\
  &= \frac{1}{4} (\langle \vec{x}, \vec{y}\rangle + \langle \vec{y}, \vec{x}\rangle + \langle \vec{x}, \vec{y}\rangle + \langle \vec{y}, \vec{x}\rangle) \\
  &= \frac{1}{4} (2\langle \vec{x}, \vec{y} \rangle + 2\langle \vec{y}, \vec{x} \rangle) \\
  &= \frac{1}{4} (4\langle \vec{x}, \vec{y} \rangle) \\
  &= 4\langle \vec{x}, \vec{y} \rangle
\end{align*}

(ii) \textit{Proof:}
\begin{align*}
  ||\vec{x}||^2 + ||\vec{y}||^2 &= \frac{1}{2}(||\vec{x} + \vec{y}||^2 + ||\vec{x} - \vec{y}||^2) \\
  &= \frac{1}{2} (\langle \vec{x} + \vec{y}, \vec{x} + \vec{y} \rangle + \langle \vec{x} - \vec{y}, \vec{x} - \vec{y} \rangle) \\
  &= \frac{1}{2} (\langle \vec{x}, \vec{x} + \vec{y} \rangle + \langle \vec{y}, \vec{x} + \vec{y} \rangle + \langle \vec{x} - \vec{y}, \vec{x} \rangle - \langle \vec{x} - \vec{y}, \vec{y} \rangle) \\
  &= \frac{1}{2} (\langle \vec{x}, \vec{x} \rangle + \langle \vec{x}, \vec{y} \rangle + \langle \vec{y}, \vec{x} \rangle + \langle \vec{y}, \vec{y} \rangle + \langle \vec{x}, \vec{x} \rangle - \langle \vec{y}, \vec{x} \rangle - \langle \vec{x}, \vec{y} \rangle + \langle \vec{y}, \vec{y} \rangle) \\
  &= \frac{1}{2} (2\langle \vec{x}, \vec{x} \rangle + 2 \langle \vec{y}, \vec{y} \rangle) \\
  &= \langle \vec{x}, \vec{x} \rangle + 2 \langle \vec{y}, \vec{y} \rangle \\
  &= ||\vec{x}||^2 + ||\vec{y}||^2
\end{align*}

\pagebreak

\textbf{Exercise 2} \\
\textit{Proof:}
\begin{align*}
  \langle \vec{x}, \vec{y} \rangle &= \frac{1}{4}(||\vec{x} + \vec{y}||^2 - ||\vec{x} - \vec{y}||^2 + i||\vec{x} - i\vec{y}||^2 - i||\vec{x} + i\vec{y}||^2) \\
  &= \frac{1}{4}(||\vec{x} + \vec{y}||^2 - ||\vec{x} - \vec{y}||^2) + \frac{1}{4} (i||\vec{x} - i\vec{y}||^2 - i||\vec{x} + i\vec{y}||^2) \\
  &= \langle \vec{x}, \vec{y} \rangle + \frac{1}{4} (i||\vec{x} - i\vec{y}||^2 - i||\vec{x} + i\vec{y}||^2) \\
  &= \langle \vec{x}, \vec{y} \rangle + \frac{1}{4} (i \langle \vec{x} - i\vec{y}, \vec{x} - i\vec{y} \rangle - i \langle \vec{x} + i\vec{y}, \vec{x} + i\vec{y} \rangle) \\
  &= \langle \vec{x}, \vec{y} \rangle + \frac{1}{4}
  (i \langle \vec{x}, \vec{x} - i\vec{y} \rangle +  i^2 \langle \vec{y}, \vec{x} - i\vec{y} \rangle - i \langle \vec{x}, \vec{x} + i\vec{y} \rangle + (-i)^2 \langle \vec{y}, \vec{x} + i\vec{y} \rangle) \\
\begin{split}
    &= \langle \vec{x}, \vec{y} \rangle + \frac{1}{4}
    (i \langle \vec{x}, \vec{x} \rangle + i^2 \langle \vec{x}, \vec{y} \rangle + i^2 \langle \vec{y}, \vec{x} \rangle + i^3 \langle \vec{y}, \vec{y} \rangle \\
    &\qquad - i \langle \vec{x}, \vec{x} \rangle + (-i)^2 \langle \vec{x}, \vec{y} \rangle + (-i)^2 \langle \vec{y}, \vec{x} \rangle + (-i^3) \langle \vec{y}, \vec{y} \rangle)\ \\
\end{split}\\
  &= \langle \vec{x}, \vec{y} \rangle + \frac{1}{4}
  (i^2 \langle \vec{x}, \vec{y} \rangle + i^2 \langle \vec{y}, \vec{x} \rangle + (-i)^2 \langle \vec{x}, \vec{y} \rangle + (-i)^2 \langle \vec{y}, \vec{x} \rangle) \\
  &= \langle \vec{x}, \vec{y} \rangle + \frac{1}{4}
  (-\langle \vec{x}, \vec{y} \rangle - \langle \vec{y}, \vec{x} \rangle + \langle \vec{x}, \vec{y} \rangle + \langle \vec{y}, \vec{x} \rangle) \\
  &= \langle \vec{x}, \vec{y} \rangle
\end{align*}

\textbf{Exercise 3} \\
(i) $\cos \ \theta = \frac{\langle x, x^5 \rangle}{||x|| \cdot ||x^5||}$ \\
$\langle x, x^5 \rangle = \int_0^1 x \cdot x^5 \ dx = \frac{1}{7}$ \\
$\langle x, x \rangle = \int_0^1 x \cdot x \ dx = \frac{1}{3} \implies ||x|| = \frac{1}{\sqrt{3}}$ \\
$\langle x^5, x^5 \rangle = \int_0^1 x^5 \cdot x^5 \ dx = \frac{1}{11} \implies ||x^5|| = \frac{1}{\sqrt{11}}$ \\
$\implies \theta = \cos^{-1} (\frac{\sqrt{33}}{7})$ \\

(ii) $\cos \ \theta = \frac{\langle x^2, x^4 \rangle}{||x^2|| \cdot ||x^4||}$ \\
$\langle x^2, x^4 \rangle = \int_0^1 x^2 \cdot x^4 \ dx = \frac{1}{7}$ \\
$\langle x^2, x^2 \rangle = \int_0^1 x^2 \cdot x^2 \ dx = \frac{1}{5} \implies ||x^2|| = \frac{1}{\sqrt{5}}$ \\
$\langle x^4, x^4 \rangle = \int_0^1 x^4 \cdot x^4 \ dx = \frac{1}{9} \implies ||x^4|| = \frac{1}{3}$ \\
$\implies \theta = \cos^{-1} (\frac{3\sqrt{5}}{7})$ \\

\textbf{Exercise 8} \\
(i) $\langle \cos t, \sin t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \cos t \sin t \ dt = 0$
\qquad $\langle \cos t, \cos t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \cos t \cos t \ dt = 1$

$\langle \cos t, \cos 2t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \cos t \cos 2t \ dt = 0$
\qquad $\langle \sin t, \sin t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \sin t \sin t \ dt = 1$

$\langle \cos t, \sin 2t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \cos t \sin 2t \ dt = 0$
\qquad $\langle \cos 2t, \cos 2t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \cos 2t \cos 2t \ dt = 1$

$\langle \sin t, \cos 2t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \sin t \cos 2t \ dt = 0$
\qquad $\langle \sin 2t, \sin 2t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \sin 2t \sin 2t \ dt = 1$

$\langle \sin t, \sin 2t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \cos t \sin 2t \ dt = 0$

$\langle \cos 2t, \sin 2t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi \cos 2t \sin 2t \ dt = 0$ \\

(ii) $||t|| = \langle t, t \rangle = \frac{1}{\pi} \int_{-\pi}^\pi t^2 \ dt = \frac{2\pi^2}{3}$ \\

(iii)
\begin{align*}
  \begin{split}
    proj_X(\cos 3t) &= \langle \cos t, \cos 3t \rangle \cdot \cos t + \langle \sin t, \cos 3t \rangle \cdot \sin t \\
    & \qquad \qquad + \langle \cos 2t, \cos 3t \rangle \cdot \cos 2t + \langle \sin 2t, \cos 3t \rangle \cdot \sin 2t
  \end{split} \\
  \begin{split}
    &= \Big(\frac{1}{\pi} \int_{-\pi}^\pi \cos t \cos 3t \ dt \Big) \cdot \cos t +
    \Big(\frac{1}{\pi} \int_{-\pi}^\pi \sin t \cos 3t \ dt \Big) \cdot \sin t \\
    & \qquad \qquad + \Big(\frac{1}{\pi} \int_{-\pi}^\pi \cos 2t \cos 3t \ dt \Big) \cdot \cos 2t +
    \Big(\frac{1}{\pi} \int_{-\pi}^\pi \sin 2t \cos 3t \ dt \Big) \cdot \sin 2t
  \end{split} \\
  &= 0
\end{align*}

(iv)
\begin{align*}
  \begin{split}
    proj_X(t) &= \langle \cos t, t \rangle \cdot \cos t + \langle \sin t, t \rangle \cdot \sin t \\
    & \qquad \qquad + \langle \cos 2t, t \rangle \cdot \cos 2t + \langle \sin 2t, t \rangle \cdot \sin 2t
  \end{split} \\
  \begin{split}
    &= \Big(\frac{1}{\pi} \int_{-\pi}^\pi (\cos t) \ t \ dt \Big) \cdot \cos t +
    \Big(\frac{1}{\pi} \int_{-\pi}^\pi (\sin t) \ t \ dt \Big) \cdot \sin t \\
    & \qquad \qquad + \Big(\frac{1}{\pi} \int_{-\pi}^\pi (\cos 2t) \ t \ dt \Big) \cdot \cos 2t +
    \Big(\frac{1}{\pi} \int_{-\pi}^\pi (\sin 2t) \ t \ dt \Big) \cdot \sin 2t
  \end{split} \\
  &= 2\sin t - \sin(2t)
\end{align*}

\textbf{Exercise 9} \\
The rotation matrix $R_\mathcal{\theta}$ is orthonormal since $R_\mathcal{\theta}R_\mathcal{\theta}^H=I$:
\[
\begin{bmatrix}
  \cos \theta & -\sin \theta \\
  \sin \theta & \cos \theta
\end{bmatrix}
\begin{bmatrix}
  \cos \theta & \sin \theta \\
  -\sin \theta & \cos \theta
\end{bmatrix}
=
\begin{bmatrix}
  \cos^2 \theta + \sin^2 \theta & \cos \theta \sin \theta - \sin \theta \cos \theta \\
  \cos \theta \sin \theta - \sin \theta \cos \theta & \cos^2 \theta + \sin^2 \theta
\end{bmatrix}
=
\begin{bmatrix}
  1 & 0 \\
  0 & 1
\end{bmatrix}
\]

\textbf{Exercise 10} \\
(i) $Q \in M_n(\mathbb{F}^n)$ is an orthonormal matrix if and only if $Q^HQ=QQ^H=I$. \\
\textit{Proof:} \\
Let $Q$ be an orthonormal matrix. Then we have that $\langle \vec{x}, \vec{y} \rangle = \langle Q\vec{x},  Q\vec{y} \rangle = (Q\vec{x})^HQ\vec{y} = \vec{x}^HQ^HQ\vec{y}$. Since we know that $\langle \vec{x}, \vec{y} \rangle = \vec{x}^H\vec{y}$, it must be that $Q^HQ = I$. Also, $QQ^H = I$ since $Q$ is invertible.\\

Now suppose that for a matrix $Q$, we have that $Q^HQ = QQ^H = I$. Consider the inner product $\langle Q\vec{x},  Q\vec{y} \rangle = (Q\vec{x})^HQ\vec{y} = \vec{x}^HQ^HQ\vec{y} = \vec{x}^H\vec{y} = \langle \vec{x}, \vec{y} \rangle$. Therefore, $Q$ must be an orthonormal matrix. \\

(ii) If $Q \in M_n(\mathbb{F})$ is an orthonormal matrix, then $||Q\vec{x}|| = ||\vec{x}||$ for all $\vec{x} \in \mathbb{F}^n$. \\
\textit{Proof:} \\
$||Q\vec{x}|| = \sqrt{\langle Q\vec{x},  Q\vec{y} \rangle} = \sqrt{\langle \vec{x},  \vec{y} \rangle} = ||\vec{x}||$. \\

(iii) If $Q \in M_n(\mathbb{F}^n)$ is an orthonormal matrix, then so is $Q^{-1}$. \\
\textit{Proof:} \\
Assume $Q$ is an orthonormal matrix. Then we know that $Q^HQ=QQ^H=I$, so $Q^{-1} = Q^H$. To prove that $Q^H$ is orthonormal, we can show that $(Q^H)^H Q^H = Q^H (Q^H)^H = I$. \\

(iv) The columns of an orthonormal matrix $Q \in M_n(\mathbb{F}^n)$ are orthonormal. \\
\textit{Proof:} \\
Assume that $Q$ is orthonormal. Consider standard basis vectors $e_i, e_j \in \mathbb{F}^n$. The $i$th column of $Q$ is $Q \vec{e_i}$. Since $Q$ is orthonormal, we have that $\langle Q \vec{e_i}, Q \vec{e_j} \rangle = \langle e_i, e_j \rangle = \delta_{ij}$. Therefore the columns of $Q$ must be orthonormal. \\

(v) If $Q \in M_n(\mathbb{F}^n)$ is an orthonormal matrix, then $|det(Q)| = 1$.
\textit{Proof:} \\
Since $Q$ is orthonormal, we know that $QQ^H = I$. Then $det(QQ^H) = det(I) = 1$. By the properties of determinants, we have that $det(QQ^H) = det(Q) \cdot det(Q^H) = 1$. Using the fact that $Q$ and $Q^H$ have the same determinants, we conclude that $|det(Q)| = 1$.

No. Consider the matrix
\[
\begin{bmatrix}
  1 & 1 \\
  0 & 1
\end{bmatrix}
\] \\

(vi) If $Q_1, Q_2 \in M_n(\mathbb{F}^n)$ are orthonormal matrices, then the product $Q_1Q_2$ is also an orthonormal matrix. \\
\textit{Proof:} \\
$(Q_1Q_2)^HQ_1Q_2 = Q_2^H Q_1^H Q_1 Q_2 = I$ \\
$Q_1Q_2(Q_1Q_2)^H = Q_1 Q_2 Q_2^H Q_1^H = I$. \\
Therefore $Q_1Q_2$ is an orthonormal matrix. \\

\textbf{Exercise 11} \\
Let $\{\vec{x}_1, \vec{x}_2, ..., \vec{x}_n\}$ be a linearly dependent set. Assume that the vector $\vec{x}_k$ is linearly dependent on $\{\vec{x}_1, \vec{x}_2, ..., \vec{x}_{k-1}\}$, i.e. $\vec{x}_k \in X = Span(\{\vec{x}_1, \vec{x}_2, ..., \vec{x}_{k-1}\})$. When we apply the Gram-Schmidt orthonormalization process, we have that $\vec{p}_{k-1} = proj_X (\vec{x}_k) = \vec{x}_k \implies \vec{q}_k = \vec{0}$. If we remove all the zero vectors from the set, we have an orthonormal set of linearly dependent vectors. \\

\textbf{Exercise 16} \\
(i) Let $Q_1R_1$ and $Q_2R_2$ be distinct $QR$ decompositions of a matrix $A$ s.t. $Q_2 = Q_1D$ and $R_2 = D^{-1}R_2$ where $D$ is a diagonal matrix with all its diagonal entries $\pm 1$. Then $Q_2$ is orthonormal since $Q_1$ is orthonormal. Also, $R_2$ is upper triangular since $R_1$ is upper triangular. \\

(ii) Assume that $A = Q_1R_1 = Q_2R_2$ s.t. $Q_i$ is orthonormal and $R_i$ is upper triangular with only positive diagonal entries. Thus $R_i$ is invertible and $Q_iQ_i^H=I$. Then we have that $R_1R_2^{-1} = Q_1^HQ_2$. Since the $R_i$ are upper triangular with only positive entries on the diagonal, $R_1R_2^{-1}$ must be upper triangular with only positive entries on the diagonal as well. Also, since the $Q_i$ are orthonormal, $Q_1^HQ_2$ must be orthonormal. Then, $R_1R_2^{-1} = Q_1^HQ_2 = I$, so $R_1=R_2$ and $Q_1=Q_2$. \\

\textbf{Exercise 17} \\
Let $A = \hat{Q}\hat{R}$. \\
$A^HA\vec{x} = A^H\vec{b}$ \\
$\implies (\hat{Q}\hat{R})^H\hat{Q}\hat{R}\vec{x}= (\hat{Q}\hat{R})^H\vec{b}$ \\
$\implies \hat{R}^H\hat{Q}^H\hat{Q}\hat{R}\vec{x}= \hat{R}^H\hat{Q}^H\vec{b}$ \\
$\implies \hat{Q}\hat{R}\vec{x}= \vec{b}$ \\
$\implies \hat{R}\vec{x}= \hat{Q}^H\vec{b}$ \\

\textbf{Exercise 23} \\
By the triangle inequality, \\
\begin{equation*}
  ||\vec{x}|| = ||(\vec{x} - \vec{y}) + \vec{y}|| \leq ||\vec{x} - \vec{y}|| + ||\vec{y}||
  \Leftrightarrow ||\vec{x}|| - ||\vec{y}|| \leq ||\vec{x} - \vec{y}||.
\end{equation*}
Similarly and by scale preservation, \\
\begin{equation*}
  ||\vec{y}|| = |-1| \cdot ||\vec{y}|| = ||-\vec{y}|| = ||(\vec{x} - \vec{y}) + \vec{x}|| \leq ||\vec{x} - \vec{y}|| + ||\vec{y}||
  \Leftrightarrow ||\vec{y}|| - ||\vec{x}|| \leq ||\vec{x} - \vec{y}||
\end{equation*}
\begin{equation*}
  \implies |||\vec{x}|| - ||\vec{y}||| \leq ||\vec{x} - \vec{y}||
\end{equation*}

\textbf{Exercise 24} \\
(i) $||f||_{L^1} = \int_a^b |f(t)| dt$ \\
1. Positivity: $|f(t)| \geq 0$ for all $x \implies \int_a^b |f(t)|dt \geq 0$. \\
Also, $\int_a^b |f(t)|dt = 0$ if and only if $|f(t)| = 0$. \\

2. Scale preservation: $||af||_{L^1} = \int_a^b |af(t)| dt = |a| \int_a^b |f(t)| dt = |a| ||f||_{L^1}$ \\

3. Triangle inequality: Consider $f, g \in C[a, b]$.
\begin{align*}
  || f + g ||_{L^1} &= \int_a^b |f(t) + g(t)| dt \\
  &\leq \int_a^b (|f(t)| + |g(t)|) dt \\
  &= \int_a^b |f(t)|dt + \int_a^b |g(t)|dt \\
  &= ||f||_{L^1} + ||g||_{L^1}
\end{align*}

(ii) $||f||_{L^2} = (\int_a^b |f(t)|^2 dt)^{\frac{1}{2}}$ \\
1. Positivity: $|f(t)|^2 \geq 0$ for all $x \implies (\int_a^b |f(t)|^2dt)^\frac{1}{2} \geq 0$. \\
Also, $(\int_a^b |f(t)|^2 dt)^\frac{1}{2} = 0$ if and only if $|f(t)| = 0$. \\

2. Scale preservation: $||af||_{L^2} = (\int_a^b |af(t)|^2 dt)^\frac{1}{2} = |a| (\int_a^b |f(t)|^2 dt)^\frac{1}{2} = |a| ||f||_{L^2}$ \\

3. Triangle inequality: Consider $f, g \in C[a, b]$.
\begin{align*}
  || f + g || &= \Big(\int_a^b |f(t) + g(t)|^2 dt\Big)^\frac{1}{2} \\
  &\leq \Big(\int_a^b |f(t)|^2dt + \int_a^b |g(t)|^2dt\Big)^\frac{1}{2} \\
  &\leq ||f||_{L^2} + ||g||_{L^2}
\end{align*}

(iii) $||f||_{L^\infty} = \sup_{x \in [a,b] |f(x)|}$ \\
1. Positivity: $|f(x)| \geq 0$ for all $x \implies \sup_{x \in [a, b]} |f(x)| \geq 0$. \\
Also, $\sup_{x \in [a, b]} |f(x)| = 0$ if and only if $|f(x)| = 0$. \\

2. Scale preservation: $||af||_{L^\infty} = \sup_{x \in [a, b]} |a||f(x)| = |a| \sup_{x \in [a, b]} |f(x)| = |a|||f||_{L^\infty}$ \\

3. Triangle inequality:
  For any $x \in [a, b]$
  \begin{align*}
    |f(x) + g(x)| &\leq |f(x)| + |g(x)| \\
    &\leq \sup_{x \in [a, b]} |f(x)| + \sup_{x \in [a, b]} |g(x)| \\
    &= ||f||_{L^\infty} + ||g||_{L^\infty} \\
    &\implies \sup_{x \in [a, b]} |f(x) + g(x)| = ||f + g ||_{L^\infty} \leq ||f||_{L^\infty} + ||g||_{L^\infty}
  \end{align*}

\textbf{Exercise 26} \\
Prove that topological equivalence is an equivalence relation. We check that it satisfies the three conditions: \\

1. Reflexivity: $||\cdot||_a \sim ||\cdot||_a$ \\
We see that $m||\vec{x}||_a \leq ||\vec{x}||_a \leq M||\vec{x}||_a$ holds for a choice of constants $m=1$ and $M=2$. \\

2. Symmetry: $||\cdot||_a \sim ||\cdot||_b \implies ||\cdot||_b \sim ||\cdot||_a$ \\
If $m||\vec{x}||_a \leq ||\vec{x}||_b \leq M||\vec{x}||_a$, then the relation holds for $\frac{1}{M}||\vec{x}||_b \leq ||\vec{x}||_a \leq \frac{1}{m}||\vec{x}||_b$. \\

3. Transitivity: $||\cdot||_a \sim ||\cdot||_b \text{ and } ||\cdot||_b \sim ||\cdot||_c \implies ||\cdot||_a \sim ||\cdot||_c$ \\
If $m_1||\vec{x}||_a \leq ||\vec{x}||_b \leq M_1||\vec{x}||_a$ and $m_2||\vec{x}||_b \leq ||\vec{x}||_c \leq M_2||\vec{x}||_b$, then the relation holds for $m_1m_2||\vec{x}||_a \leq ||\vec{x}||_c \leq M_1M_2||\vec{x}||_a$. \\

(i) Show that $||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt{n} ||\vec{x}||_2$. \\
\begin{align*}
  ||\vec{x}||_1^2 &= (|x_1| + |x_2| + \cdots + |x_n|)^2 \geq |x_1|^2 + |x_2|^2 + \cdots + |x_n|^2 \\
  & \implies ||\vec{x}||_1 \geq (|x_1|^2 + |x_2|^2 + \cdots + |x_n|^2)^\frac{1}{2} = ||\vec{x}||_2
\end{align*}
Also,
\begin{align*}
  &||\vec{x}||_1 = |\langle \vec{x}, \vec{1} \rangle| \leq ||\vec{x}||_2 \cdot ||\vec{1}||_2 \text{ where } ||\vec{1}||_2 = \Big(\sum_{j=1}^n 1^2\Big)^\frac{1}{2} = \sqrt{n} \\
  & \implies ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt{n} ||\vec{x}||_2
\end{align*}

(ii) Show that $||\vec{x}||_\infty \leq ||\vec{x}||_2 \leq \sqrt{n} ||\vec{x}||_\infty$. \\
\begin{align*}
  ||\vec{x}||_\infty = \sup_i{|x_i|} = (\sup_i{|x_i|^2})^\frac{1}{2} \leq (|x_1|^2 + |x_2|^2 + \cdots + |x_n|^2)^\frac{1}{2} = ||\vec{x}||_2
\end{align*}
Also,
\begin{align*}
  &||\vec{x}||_2 = (|x_1|^2 + |x_2|^2 + \cdots + |x_n|^2)^\frac{1}{2} \leq (n |x_n|^2)^\frac{1}{2} = \sqrt{n} ||\vec{x}||_2 \\
  & \implies ||\vec{x}||_\infty \leq ||\vec{x}||_2 \leq \sqrt{n} ||\vec{x}||_\infty
\end{align*}

\textbf{Exercise 28} \\
(i) $\frac{1}{\sqrt{n}} ||A||_2 \leq ||A||_1 \leq \sqrt{n} ||A||_2$ \\
\textit{Proof:} \\
From Exercise 26(i), we know that
\begin{align*}
  &||A\vec{x}||_2 \leq ||A\vec{x}||_1 \leq \sqrt{n} ||A\vec{x}||_2 \quad \text{ and } \quad \frac{1}{\sqrt{n}} ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq ||\vec{x}||_2 \\
  & \implies \frac{1}{\sqrt{n}} \frac{||A\vec{x}||_2}{||\vec{x}||_2} \leq \frac{||A\vec{x}||_1}{||\vec{x}||_1} \leq \sqrt{n} \frac{||A\vec{x}||_2}{||\vec{x}||_2} \\
  & \implies \frac{1}{\sqrt{n}} ||A||_2 \leq ||A||_1 \leq \sqrt{n} ||A||_2
\end{align*}

(ii) $\frac{1}{\sqrt{n}} ||A||_\infty \leq ||A||_2 \leq \sqrt{n} ||A||_\infty$ \\
\textit{Proof:} \\
From Exercise 26(ii), we know that
\begin{align*}
  &||A\vec{x}||_\infty \leq ||A\vec{x}||_2 \leq \sqrt{n} ||A\vec{x}||_\infty \quad \text{ and } \quad \frac{1}{\sqrt{n}} ||\vec{x}||_\infty \leq ||\vec{x}||_2 \leq ||\vec{x}||_\infty \\
  & \implies \frac{1}{\sqrt{n}} \frac{||A\vec{x}||_\infty}{||\vec{x}||_\infty} \leq \frac{||A\vec{x}||_2}{||\vec{x}||_2} \leq \sqrt{n} \frac{||A\vec{x}||_\infty}{||\vec{x}||_\infty} \\
  & \implies \frac{1}{\sqrt{n}} ||A||_\infty \leq ||A||_2 \leq \sqrt{n} ||A||_\infty
\end{align*}

\textbf{Exercise 29} \\
Any orthonormal matrix $Q \in M_n(\mathbb{F})$ has $||Q||=1$. \\
\textit{Proof:} \\
$||Q|| = \sup_{\vec{x} \neq \vec{0}} \frac{||Q\vec{x}||_2}{||\vec{x}||_2} = \frac{||\vec{x}||_2}{||\vec{x}||_2} = 1$ \\

For any $\vec{x} \in \mathbb{F}^n$, let $R_x: M_n(\mathbb{F} \rightarrow \mathbb{F}^n)$ be the linear transformation $A \mapsto A\vec{x}$. The induced norm of the transformation $R_x$ is equal to $||\vec{x}||_2$. \\
\textit{Proof:} \\
$||R_x|| = \sup_{||A||_2 \neq 0} \frac{||A\vec{x}||_2}{||A||} = \sup_{||A||_2 \neq 0} \frac{||A\vec{x}||_2 ||\vec{x}||_2}{||A||||\vec{x}||_2} \leq \sup_{||A||_2 \neq 0} \frac{||A\vec{x}||_2 ||\vec{x}||_2}{||A\vec{x}||_2} = ||\vec{x}_2||$ \\
Since A is orthonormal, we have that $||A|| = 1$ and $||A\vec{x}||_2 = ||\vec{x}||_2 \implies ||R_x|| = ||\vec{x}_2||$. \\

\textbf{Exercise 30} \\
Let $S \in M_n(\mathbb{F})$ be an invertible matrix. Given any matrix norm $||\cdot||$ on $M_n$, define $||\cdot||_S$ by $||A||_S = ||SAS^{-1}||$. Then $||\cdot||_S$ is a matrix norm on $M_n$. \\
\textit{Proof:} \\
1. Positivity: $||SAS^{-1}|| \geq 0$ by definition. \\
Also, $||SAS^{-1}|| = 0$ if and only if $A = 0$. \\

2. Scale preservation: Let $a \in \mathbb{R}.$ Then $||aA||_S = ||aSAS^{-1}|| = |a| ||SAS^{-1}|| = |a| ||A||_S$. \\

3. Triangle inequality: Let $A_1, A_2 \in M_n$. Then $||A_1 + A_2||_S = ||S(A_1 + A_2)S^{-1}|| = ||SA_1S^{-1} + SA_2S^{-1}|| \leq ||SA_1S^{-1}|| + ||SA_2S^{-1}|| = ||A_1||_S + ||A_2||_S$. \\

4. Submultiplicative property: $||A_1A_2||_S = ||SA_1A_2S^{-1}|| = ||SA_1S^{-1}SA_2S^{-1}|| \leq ||SA_1S^{-1}|| \cdot ||SA_2S^{-1}|| = ||A_1||_S \cdot ||A_2||_S$. \\

\textbf{Exercise 37} \\
We define $\mathcal{S} = \{1, x, x^2\}$ to be the basis of the space V. Then we can evaluate $L$ on the basis vectors: $L[1] = 0, L[x] = 1, L[x^2] = 2$. Every function $p \in V$ can be written as a linear combination of the basis vectors:
\begin{equation*}
  p = a_1 + a_2 x + a_3 x^2 \implies L[p] = a_1 L[1] + a_2 L[x] + a_3 L[x^2]
\end{equation*}

\[
L[p] =
\begin{bmatrix}
  L[1] & L[x] & L[x^2]
\end{bmatrix}
\begin{bmatrix}
  a_1 \\
  a_2 \\
  a_3
\end{bmatrix}
= \langle (0, 1, 2), x \rangle
\] \\

\textbf{Exercise 38} \\
We again define $\mathcal{S} = \{1, x, x^2\}$ to be the basis of the space V, and we evaluate $D$ on the basis vectors: $D[1](x) = 0, D[x](x) = 1, D[x^2](x) = 2x$. The matrix representation of $D$ with respect to this basis is
\[
D =
\begin{bmatrix}
  0 & 1 & 0 \\
  0 & 0 & 2 \\
  0 & 0 & 0
\end{bmatrix}
\]
Using intergration by parts,
\begin{equation*}
  \langle q, D[p] \rangle = \int_{-\infty}^{\infty} q(x) p'(x) dx = - \int_{-\infty}^\infty q'(x) p(x) dx = -\langle D[q], [p] \rangle.
\end{equation*}
Therefore the matrix representation of the adjoint of D with respect to the basis is
\[
D^* = -D =
\begin{bmatrix}
  0 & -1 & 0 \\
  0 & 0 & -2 \\
  0 & 0 & 0
\end{bmatrix}
\]

\textbf{Exercise 39} \\
Let $V$ and $W$ be finite-dimensional inner product spaces. The adjoint has the following properties:

(i) If $S,T \in \mathscr{L}(V;W)$, then $(S+T)^* = S^* + T^*$ and $(\alpha T)^* = \bar{\alpha}T^*, \alpha \in \mathbb{F}$. \\
\textit{Proof:}
\begin{align*}
  \langle \vec{w}, (S+T)(\vec{v}) \rangle &= \langle \vec{w}, S(\vec{v}) \rangle + \langle \vec{w}, T(\vec{v}) \rangle \\
  &= \langle S^*(\vec{w}), \vec{v} \rangle + \langle T^*(\vec{w}), \vec{v} \rangle \\
  &= \langle (S+T)^*(\vec{w}), \vec{v} \rangle
\end{align*}
\begin{align*}
  \langle (\alpha T)(\vec{w}), \vec{v} \rangle &= \alpha \langle  T(\vec{w}), \vec{v} \rangle \\
  &= \alpha \langle \vec{w}, T^*(\vec{v}) \rangle \\
  &= \langle \vec{w}, \bar{\alpha} T^*(\vec{v}) \rangle
\end{align*}


(ii) If $S \in \mathscr{L}(V;W)$, then $(S^*)^*=S$. \\
\textit{Proof:}\\
$\langle \vec{w}, (S^*)^* (\vec{v}) \rangle = \langle S^*(\vec{w}),  \vec{v} \rangle = \langle \vec{w}, S(\vec{v}) \rangle$ \\

(iii) If $S,T \in \mathscr{L}(V)$, then $(ST)^* + T^*S^*$. \\
\textit{Proof:} \\
$\langle \vec{w}, (ST)^*(\vec{v}) \rangle = \langle (ST)(\vec{w}), \vec{v} \rangle = \langle T(\vec{w}), S^*(\vec{v}) \rangle = \langle \vec{w}, T^*S^*(\vec{v}) \rangle$ \\

(iv) If $T \in \mathscr{L}(V)$ and $T$ is invertible, then $(T^*)^{-1} = (T^{-1})^*$. \\
\textit{Proof:} \\
By (iii),
$T^*(T^*)^{-1} = (TT^{-1})^* = I^* = I$. \\

\textbf{Exercise 40} \\
Let $M_n(\mathbb{F})$ be endowed with the Frobenius inner product. Any $A \in M_n(\mathbb{F})$ defines a linear operator on $M_n(\mathbb{F})$ by left multiplication: $B \mapsto AB$.

(i) Show that $A^* = A^H$. \\
\textit{Proof:} \\
Let $B, C \in M_n$. Then the Frobenius inner product is
$\langle B, AC \rangle = tr(B^HAC) = tr((A^HB)^HC) = \langle A^HB, C \rangle$. \\

(ii) Show that for any $A_1, A_2, A_3 \in M_n(\mathbb{F})$ we have $\langle A_2, A_3A_1 \rangle = \langle A_2A_1^*, A_3 \rangle$. \\
\textit{Proof:} \\
$\langle A_2, A_3A_1 \rangle = tr(A_2^HA_3A_1) = tr(A_1A_2^HA_3) = tr((A_2A_1^H)^HA_3) = \langle A_2A_1^H, A_3 \rangle = \langle A_2A_1^*, A_3 \rangle$ by the result of (i). \\

\textbf{Exercise 44} \\
Given $A \in M_{m \times n}(\mathbb{F})$ and $\vec{b} \in \mathbb{F}^m$, either $A\vec{x} = \vec{b}$ has a solution $\vec{x} \in \mathbb{F}^n$ or there exists $\vec{y} \in \mathscr{N}(A^H)$ such that $\langle \vec{y}, \vec{b} \rangle \neq 0$ (Fredholm alternative). \\
\textit{Proof:} \\
First, we must show that if $A\vec{x} = \vec{b}$ has a solution, then $\forall \vec{y} \in \mathscr{N}(A^H)$ we have that $\langle \vec{y}, \vec{b} \rangle = 0$. We know that $\mathscr{N}(A^H)$ is orthogonal to $\mathscr{R}(A)$ so $\forall \vec{y} \in \mathscr{N}(A^H)$ and $\forall \vec{b} \in \mathscr{R}(A)$, we have that $\langle \vec{y}, \vec{b} \rangle = 0$ by definition. \\
On the other hand, we want to show that if $\exists \vec{y} \in \mathscr{N}(A^H)$ s.t. $\langle \vec{y}, \vec{b} \rangle \neq 0$, then $A\vec{x} = \vec{b}$ has no solution. This follows similarly to the proof of the first claim; if $\exists \vec{y} \in \mathscr{N}(A^H)$  s.t. $\langle \vec{y}, \vec{b} \rangle \neq 0$ then $b \notin \mathscr{R}(A)$, so $A\vec{x} = \vec{b}$ must have no solution. \\

\textbf{Exercise 45} \\
Consider the vector space $M_n(\mathbb{R})$ with the Frobenius inner product. Show that $Sym_n(\mathbb{R})^\perp = Skew_n(\mathbb{R})$. \\
\textit{Proof:} \\

\textbf{Exercise 46} \\
Let $A$ be an $m \times n$ matrix. \\

(i) If $\vec{x} = \mathscr{N}(A^HA)$, then $A\vec{x}$ is in both $\mathscr{R}(A)$ and $\mathscr{N}(A^H)$. \\
\textit{Proof:} \\
By definition, $A\vec{x} \in \mathscr{R}(A)$. Additionally, $A^HA\vec{x} = \vec{0}$, so $A\vec{x} \in \mathscr{N}(A^H)$. \\

(ii) $\mathscr{N}(A^HA) = \mathscr{N}(A)$. \\
\textit{Proof:} \\
Let $\vec{x} \in \mathscr{N}(A)$. Then $A\vec{x} = \vec{0} \implies A^HA\vec{x} = \vec{0}$. Now let $\vec{x} \in \mathscr{N}(A^HA)$. Then $A^HA\vec{x} = \vec{0}$. To show that $A\vec{x} = \vec{0}$, we take its norm $||A\vec{x}||^2 = \langle A\vec{x}, A\vec{x} \rangle = \vec{x}^HA^HA\vec{x} = 0$. Recall that this happens if and only if $A\vec{x} = \vec{0}$. \\

(iii) $A$ and $A^HA$ have the same rank. \\
\textit{Proof:} \\
Let $rank(A)=r$. Then we know that $rank(A^H)=r$. Since $rank(A^HA) \leq \min\{rank(A), rank(A^H)\} = r$. By part (ii), we know that $dim(\mathscr{N}(A^HA)) = dim(\mathscr{N}(A))$, so $rank(A)=rank(A^H)$. \\

(iv) If $A$ has linearly independent columns, the $A^HA$ is nonsingular. \\
\textit{Proof:} \\
If $A$ has linearly independent columns, then $rank(A) = rank(A^HA) = n$. Since we know that $A^HA$ has full rank, it must be nonsingular. \\

\textbf{Exercise 47} \\
Let $P = A(A^H A)^{-1}A^H$ \\
(i) $P^2 = P$ \\
\textit{Proof:}
\begin{align*}
  P^2 &= A(A^H A)^{-1} A^H A (A^H A)^{-1} A^H \\
  &= A I_n (A^H A)^{-1} A^H \\
  &= A (A^H A)^{-1} A^H \\
\end{align*}

(ii) $P^H = P$ \\
\textit{Proof:}
\begin{align*}
  P^H &= (A(A^H A)^{-1}A^H)^H \\
  &= A ((A^H A)^{-1})^H A^H \\
  &= A ((A^H A)^H)^{-1} A^H \\
  &= A (A^H A)^{-1} A^H \\
\end{align*}
(iii) $rank(P) = n$ \\
\textit{Proof:} \\
Since $P$ is idempotent, so
$tr(P) = tr(A(A^HA)^{-1}A^H) = tr((A^HA)^{-1}A^HA) = tr(I_n) = n = rank(P)$ \\

\textbf{Exercise 48} \\
Consider the vector space $M_n(\mathbb{R})$ with the Frobenius inner product. Let $P(A) = \frac{A+A^T}{2}$ be the map $P: M_n(\mathbb{R}) \rightarrow M_n(\mathbb{R})$. Then \\

(i) $P$ is linear. \\
\textit{Proof:} \\
Let $A, B \in M_n(\mathbb{R}), \alpha, \beta \in \mathbb{R}$. \\
\begin{align*}
  P(\alpha A + \beta B) &= \frac{(\alpha A + \beta B) + (\alpha A + \beta B)^T}{2} \\
  &= \frac{\alpha A + \beta B + \alpha A^T + \beta B^T}{2} \\
  &= \alpha \frac{A + A^T}{2} + \beta \frac{B + B^T}{2}
\end{align*}

(ii) $P^2 = P$ \\
\textit{Proof:} \\
Let $A \in M_n(\mathbb{R})$. \\
$P^2 = \frac{\frac{A+A^T}{2}+(\frac{A+A^T}{2})^T}{2} = \frac{A+A^T}{2} = P$ \\

(iii) $P^* = P$ \\
\textit{Proof:} \\
Let $A, B \in M_n(\mathbb{R})$. \\
\begin{align*}
  \langle A, P(B) \rangle &= tr(A^TP(B)) \\
  &= tr\Big(A^T\frac{B + B^T}{2}\Big) \\
  &= tr\Big(A^T\frac{B}{2} + A^T\frac{B^T}{2}\Big) \\
  &= tr\Big(\frac{A^T}{2}B + \frac{A^T}{2}B^T\Big) \\
  &= tr\Big(\frac{A^T}{2}B\Big) + tr(\frac{A^T}{2}B^T\Big) \\
  &= tr\Big(\frac{A}{2}B^T\Big) + tr(\frac{A^T}{2}B^T\Big) \\
  &= tr\Big(\frac{A + A^T}{2}B^T\Big) \\
  &= tr\Big(\Big(\frac{A + A^T}{2}\Big)^TB) \\
  &= \langle P(A), B \rangle\\
\end{align*}

(iv) $\mathscr{N}(P) = Skew_n(\mathbb{R})$ \\
\textit{Proof:} \\
First suppose that $A \in \mathscr{N}(P)$. Then we know that \\
$P(A) = \frac{A + A^T}{2} = 0 \implies A^T = -A$. Therefore $A \in Skew_n(\mathbb{R})$. \\
Now assume that $A \in Skew_n(\mathbb{R})$. Then $A^T = A \implies \frac{A + A^T}{2} = \frac{A - A}{2} = 0$. Therefore $A \in \mathscr{N}(P)$, and so $\mathscr{N}(P) = Skew_n(\mathbb{R})$. \\

(v) $\mathscr{R}(P) = Sym_n(\mathbb{R})$ \\
\textit{Proof:} \\
First suppose that $A \in \mathscr{R}(P)$. Then $\exists B \in M_n(\mathbb{R})$ s.t. $P(B) = \frac{B + B^T}{2} = A$. We know that $A^T = (\frac{B + B^T}{2})^T = \frac{B + B^T}{2} = A,$ and therefore, $A \in Sym_n(\mathbb{R})$. \\
Now suppose that $A \in Sym_n(\mathbb{R})$. Then we know that $A = A^T$. Then $P(A) = \frac{A + A^T}{2} = \frac{2A}{2} = A, so A \in \mathscr{R}(P)$, and therefore $\mathscr{R}(P) = Sym_n(\mathbb{R})$. \\

(v) $||A - P(A)||_F = \sqrt{\frac{tr(A^TA) - tr(A^2)}{2}}$ \\
\textit{Proof:} \\
\begin{align*}
  ||A - P(A)||_F &= \sqrt{tr((A - P(A))^T(A - P(A)))} \\
  &= \sqrt{tr((A^T - P(A))(A - P(A)))} \\
  &= \sqrt{tr\Big(\Big(A^T - \frac{A + A^T}{2}\Big)\Big(A - \frac{A + A^T}{2}\Big)\Big)} \\
  &= \sqrt{tr\Big(\Big(\frac{A^T - A}{2}\Big)\Big(\frac{A - A^T}{2}\Big)\Big)} \\
  &= \sqrt{tr\Big(\Big(\frac{A^TA - A^2 + AA^T - (A^T)^2}{4}\Big)\Big)} \\
  &= \sqrt{\frac{tr(A^TA) - tr(A^2)}{2}}
\end{align*}

\textbf{Exercise 50} \\
Let $(x_i, y_i)_{i=1}^n$ be a collection of data points lying roughly on an ellipse of the form $rx^2 + sy^2 = 1$. Find the least squares approximation for $r$ and $s$. Write $A, \vec{x}$, and $\vec{b}$ for the corresponding normal equation in terms of the data $x_i$ and $y_i$ and the unknowns $r$ and $s$. \\

Rewriting the ellipse equation, we get $rx^2 + sy^2 = 1 \implies \frac{1}{s} - \frac{r}{s}x^2 = y^2$. We can estimate this system using OLS on the following matrices:
\[ A =
\begin{bmatrix}
  1 & x_1^2 \\
  1 & x_2^2 \\
  \vdots & \vdots \\
  1 & x_n^2 \\
\end{bmatrix}, \quad
\vec{x} =
\begin{bmatrix}
  \frac{1}{s} \\
  -\frac{r}{s}
\end{bmatrix}, \quad
\vec{b} =
\begin{bmatrix}
  y_1 \\
  y_2 \\
  \vdots \\
  y_n
\end{bmatrix}
\]
The cooresponding normal equation for the system is $A^HA \hat{\vec{x}} = A^H \vec{b}$.


\end{document}
