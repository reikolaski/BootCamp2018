\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{listings}
\lstset{frame=single,
  language=Python,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{harvard}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\usepackage{mathrsfs}
\let\vec\mathbf
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\setlength{\parindent}{0pt}

\begin{document}

\begin{flushleft}
  \textbf{\large{Problem Set \#6}} \\
  Reiko Laski
\end{flushleft}

\textbf{Exercise 9.1} \\
An unconstrained linear objective function is either constant or has no minimum. \\
\textit{Proof:} \\
Consider the unconstrained linear objective function $f(\vec{x})=\vec{b}^T\vec{x} + c$. By the FONC, we know that if a minimum exists, it will occur when $Df(\vec{x}) = \vec{0}$. If $f(\vec{x})$ is a constant function, then $Df(\vec{x}) = \vec{0}$ and we have a minimum. If $f(\vec{x})$ is not a constant function, then $Df(\vec{x}) = \vec{b}^T$ and there is no minimum. \\

\textbf{Exercise 9.2} \\
If $\vec{b} \in \mathbb{R}^m$ and $A \in M_{m \times n}(\mathbb{R})$, then the problem of finding an $\vec{x}^* \in \mathbb{R}^n$ to minimize $\|A\vec{x} - \vec{b}\|_2$ is equivalent to minimizing
\begin{align*}
  \vec{x}^TA^TA\vec{x} - 2\vec{b}^TA\vec{x}.
\end{align*}
\textit{Proof:}
\begin{align*}
  \|A\vec{x} - \vec{b}\| &= (A\vec{x} - \vec{b})^T(A\vec{x} - \vec{b})
  \\
  &= (\vec{x}^TA^T - \vec{b}^T)(A\vec{x} - \vec{b})
  \\
  &= \vec{x}^TA^TA\vec{x} - \vec{b}^TA\vec{x} -\vec{x}^TA^T\vec{b} + \vec{b}^T\vec{b}
  \\
  &= \vec{x}^TA^TA\vec{x} - 2\vec{b}^TA\vec{x} + \vec{b}^T\vec{b}
\end{align*}
The FOC of this system is equivalent to that of $\vec{x}^TA^TA\vec{x} - 2\vec{b}^TA\vec{x}$,
\begin{align*}
  2A^TA\vec{x} - 2A^T\vec{b} &= \vec{0}
  \\
  \implies A^TA\vec{x} &= A^T\vec{b}
\end{align*}
Since $A^TA$ is positive definite, the solution to the normal equation is the unique minimizer of $\|A\vec{x} - \vec{b}\|_2$. \\

\textbf{Exercise 9.3} \\
\textbf{Gradient descent} \\
(i) Basic idea: at each iteration, move in the direction $-Df^T(\vec{x}_i)$\\
(ii) Types of optimization problems that can/cannot be solved: can be used to get closer to $\vec{x}^*$ if $\vec{x}_0$ is not close enough; objective function must be differentiable \\
(iii) Relative strengths: \\
(iv) Relative weaknesses: $\alpha$ must be chosen so as not to over- or undershoot the minimum; converges slowly for problems with large condition number \\

\textbf{Newton and Quasi-Newton Methods} \\
(i) Basic idea: approximates $f(\vec{x})$ by its degree-two Taylor polynomial near $\vec{x}_k$\\
(ii) Types of optimization problems that can/cannot be solved: \\
(iii) Relative strengths: converges quadratically; reaches the opitmizer from any starting point in just one iteration if $f$ is a quadratic function of the form $f(\vec{x}) = \frac{1}{2}\vec{x}^TQ\vec{x} + \vec{b}^T\vec{x} + c$, with $Q$ symmetric and positive definite; Quasi-Newton methods have reduced computational cost of each iteration than Newton methods \\
(iv) Relative weaknesses: difficulty converging (in general) when the initial point $\vec{x}_0$ is far from $\vec{x}^*$; requires that $Df^2(\vec{x}_i)$ be positive definite; for large $n$, $(D^2f(\vec{x}_i))^{-1}Df^T(\vec{x}_i)$ is expensive, unstable, or difficult to compute; Quasi-Newton methods have worse convergence rate than Newton methods \\

\textbf{Conjugate gradient} \\
(i) Basic idea: moves towards the minimizer of a function by moving along $Q$-conjugate directions; moving in this way allows each step to be computed relatively cheaply without needing to retain much information from previous steps \\
(ii) Types of optimization problems that can/cannot be solved: work well when for large quadratic optimization problems where $Q$ is symmetric, positive definite, and sparse \\
(iii) Relative strengths: guaranteed to optimize a quadratic of $n$ variables in $n$ steps, which are generally much less expensive than the steps of Newton's \\
(iv) Relative weaknesses: may take many steps to converge \\

\textbf{Exercise 9.4} \\
Let $f(\vec{x}) = \frac{1}{2}\vec{x}^TQ\vec{x} - \vec{b}^T\vec{x}$, where $Q \in M_n(\mathbb{R})$ satisfies $Q>0$ and $\vec{b} \in \mathbb{R}^n$. The Method of Steepest Descent (that is, gradient descent with optimal line search), converges in one step (that is, $\vec{x}_1 = Q^{-1}\vec{b}$), if and only if $\vec{x}_0$ is chosen such that $Df(\vec{x}_0)^T = Q\vec{x}_0 - \vec{b}$ is an eigenvector of $Q$ \big(and $\alpha_0 = \frac{Df(\vec{x}_0)Df(\vec{x}_0)^T}{Df(\vec{x}_0)QDf(\vec{x}_0)^T}$\big). \\
\textit{Proof:} \\
First, suppose that $\vec{x}_0$ is chosen such that $Df(\vec{x}_0)^T = Q\vec{x}_0 - \vec{b}$ is an eigenvector of $Q$. Then we have that $Q(Q\vec{x}_0 - \vec{b}) = \lambda(Q\vec{x}_0 - \vec{b})$ for some $\lambda \in \mathbb{R}$. We can then evaluate $\vec{x}_1$ as\\
\begin{align*}
  \vec{x}_1 &= \vec{x}_0 - \alpha_0 Df(\vec{x}_0)^T
  \\
  &= \vec{x}_0 - \frac{Df(\vec{x}_0)Df(\vec{x}_0)^T}{Df(\vec{x}_0)QDf(\vec{x}_0)^T} Df(\vec{x}_0)^T
  \\
  &= \vec{x}_0 - \frac{(Q\vec{x}_0 - \vec{b})^T(Q\vec{x}_0 - \vec{b})}{(Q\vec{x}_0 - \vec{b})^TQ(Q\vec{x}_0 - \vec{b})} (Q\vec{x}_0 - \vec{b})
  \\
  &= \vec{x}_0 - \frac{(Q\vec{x}_0 - \vec{b})^T(Q\vec{x}_0 - \vec{b})}{(Q\vec{x}_0 - \vec{b})^T\lambda(Q\vec{x}_0 - \vec{b})} (Q\vec{x}_0 - \vec{b})
  \\
  &= \vec{x}_0 - \frac{1}{\lambda}(Q\vec{x}_0 - \vec{b})
  \\
  &= \vec{x}_0 - Q^{-1}(Q\vec{x}_0 - \vec{b})
  \\
  &= \vec{x}_0 - Q^{-1}Q\vec{x}_0 - Q^{-1}\vec{b}
  \\
  &= Q^{-1}\vec{b}
\end{align*}
Now suppose that the Method of Steepest Descent converges in one step ($\vec{x}_1 = Q^{-1}\vec{b}$). Then
\begin{align*}
  &\vec{x}_1 = \vec{x}_0 - \alpha_0 Df(\vec{x}_0)^T = \vec{x}_0 - \alpha_0 (Q\vec{x}_0 - \vec{b})
  \\
  &\implies Q^{-1}\vec{b} = \vec{x}_0 - \alpha_0 (Q\vec{x}_0 - \vec{b})
  \\
  &\implies \vec{b} = Q\vec{x}_0 - \alpha_0 Q(Q\vec{x}_0 - \vec{b})
  \\
  &\implies Q(Q\vec{x}_0 - \vec{b}) = \frac{1}{\alpha_0}(Q\vec{x}_0 - \vec{b})
\end{align*}.
Thus $\vec{x}_0$ must have been chosen such that $Df(\vec{x}_0)^T = Q\vec{x}_0 - \vec{b}$ is an eigenvector of $Q$. \\

\textbf{Exercise 9.5} \\
Assume that $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is $C^1$. Let $\{\vec{x}_k\}_{k=0}^\infty$ be defined by the Method of Steepest Descent. Then $\vec{x}_{k+1} - \vec{x}_k$ is orthogonal to $\vec{x}_{k+2} - \vec{x}_{k+1}$ for each $k$. \\
\textit{Proof:} \\
In each step of the Method of Steepest Descent, we minimize
\begin{align*}
  \phi_k(\alpha_k) = f(\vec{x}_k - \alpha_k Df(\vec{x}_k)^T)
\end{align*}
By the FONC, we have that
\begin{align*}
  Df(\vec{x}_k - \alpha_k Df(\vec{x}_k)^T)Df(\vec{x}_k)^T = \vec{0}.
\end{align*}
Note that $\vec{x}_{k+1} - \vec{x}_k = -\alpha_k Df(\vec{x}_k)^T$ and $\vec{x}_{k+2} - \vec{x}_{k+1} = -\alpha_{k+1} Df(\vec{x}_{k+1})^T$. Then we have that
\begin{align*}
  \langle \vec{x}_{k+2} - \vec{x}_{k+1}, \vec{x}_{k+1} - \vec{x}_k \rangle
  &=
  (\vec{x}_{k+2} - \vec{x}_{k+1})^T (\vec{x}_{k+1} - \vec{x}_k) \\
  &=
  (-\alpha_{k+1} Df(\vec{x}_{k+1})^T)^T (-\alpha_k Df(\vec{x}_k)^T) \\
  &=
  \alpha_{k+1}\alpha_k Df(\vec{x}_{k+1})  Df(\vec{x}_k)^T \\
  &=
  \alpha_{k+1}\alpha_k Df(\vec{x}_k - \alpha_k Df(\vec{x}_k)^T) Df(\vec{x}_k)^T \\
  &= \vec{0}
\end{align*}

\textbf{Exercise 9.6} \\
\textit{See Jupyter Notebook} \\

\textbf{Exercise 9.7} \\
\textit{See Jupyter Notebook} \\

\textbf{Exercise 9.8} \\
\textit{See Jupyter Notebook} \\

\textbf{Exercise 9.9} \\
\textit{See Jupyter Notebook} \\

\textbf{Exercise 9.10} \\
Consider the quadratic function $f(\vec{x})=\frac{1}{2}\vec{x}^TQ\vec{x} - \vec{b}^T\vec{x}$, where $Q \in M_n(\mathbb{R})$ is symmetric and positive definite and $\vec{b} \in \mathbb{R}^n$. For any initial guess $\vec{x}_0 \in \mathbb{R}^n$, one iteration of Newton's method lands at the unique minimizer of $f$. \\
\textit{Proof:} \\
Since $Q$ is positive definite, we know that there is a unique minimizer of $f(\vec{x})=\frac{1}{2}\vec{x}^TQ\vec{x} - \vec{b}^T\vec{x}$. By the FONC, $Q\vec{x}^* - \vec{b} = \vec{0} \implies \vec{x}^* = Q^{-1}\vec{b}$. Using Newton's method with an arbitrary $\vec{x}_0$, we have
\begin{align*}
  \vec{x}_1 &= \vec{x}_0 - D^2f(\vec{x}_0)^{-1}Df(\vec{x}_0)^T
  \\
  &= \vec{x}_0 - Q^{-1}(Q\vec{x}_0 - \vec{b})
  \\
  &= \vec{x}_0 - Q^{-1}Q\vec{x}_0 + Q^{-1}\vec{b}
  \\
  &= Q^{-1}\vec{b}
  \\
  &= \vec{x}^*
\end{align*}

\textbf{Exercise 9.12} \\
If $A \in M_n(\mathbb{F})$ has eigenvalues $\lambda_1, ..., \lambda_n$ and $B = A + \mu I$, then the eigenvectors of $A$ and $B$ are the same, and the eigenvalues of $B$ are $\mu + \lambda_1, \mu + \lambda_2, ..., \mu + \lambda_n$. \\
\textit{Proof:} \\
Let $\vec{x}_i$ be the eigenvector of $A$ corresponding to the eigenvalue $\lambda_i$. Then we have that
\begin{align*}
  B\vec{x}_i &= (A+\mu I)\vec{x}_i \\
  &= A\vec{x}_i + \mu I\vec{x}_i \\
  &= \lambda_i\vec{x}_i + \mu\vec{x}_i \\
  &= (\lambda_i + \mu)\vec{x}_i
\end{align*}

\textbf{Exercise 9.15} \\
Let $A$ be a nonsingular $n \times n$ matrix, $B$ an $n \times \ell$ matrix, $C$ a nonsingular $\ell \times \ell$ matrix, and $D$ an $\ell \times n$ matrix. We have
\begin{align*}
  (A + BCD)^{-1} = A^{-1} - A^{-1} B(C^{-1} + DA^{-1}B)^{-1}DA^{-1}
\end{align*}
\textit{Proof:} \\
The following is Matt's code:
\begin{align*}
  &(A+BCD)(A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1}) \\
  &= AA^{-1} - AA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} + BCDA^{-1} - BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} \\
  &= I - B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} + BCDA^{-1} - BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} \\
  &= I - B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} + BCDA^{-1} - BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} \\
  &= I + BCDA^{-1} - (B(C^{-1} + DA^{-1}B)^{-1} + BCDA^{-1}B(C^{-1} + DA^{-1}B)^{-1})DA^{-1} \\
  &= I + BCDA^{-1} - ((B+BCDA^{-1}B)(C^{-1} + DA^{-1}B)^-{1}))DA^{-1} \\
  &= I + BCDA^{-1} - (BC(C^{-1}+DA^{-1}B)(C^{-1} + DA^{-1}B)^-{1}))DA^{-1} \\
  &= I + BCDA^{-1} - BCDA^{-1} \\
  &= I
\end{align*}

\textbf{Exercise 9.16} \\
\textit{Proof:} \\
The Quasi-Newton method gives us the approximation
\begin{align*}
  A_{k+1} = A_k + \frac{\vec{y}_k - A_k\vec{s}_k}{\|\vec{s}_k\|^2}\vec{s}_k^T
\end{align*}
Let $A = A_k, B = \vec{y}_k - A_k\vec{s}_k, C = \frac{1}{\|\vec{s}_k\|^2}, D = \vec{s}_k^T$.
\begin{align*}
  A_{k+1} = A + BCD
  \\
  \implies A_{k+1}^{-1} &= (A + BCD)^{-1}
  \\
  &= A^{-1} - A^{-1} B(C^{-1} + DA^{-1}B)^{-1}DA^{-1}
  \\
  &= A_k^{-1} - \frac{A_k^{-1} B DA_k^{-1}}{C^{-1} + DA^{-1}B}
  \\
  &= A_k^{-1} - \frac{A_k^{-1} (\vec{y}_k - A_k\vec{s}_k) \vec{s}_k^TA_k^{-1}}{\|\vec{s}_k\|^2 + \vec{s}_k^TA_k^{-1}(\vec{y}_k - A_k\vec{s}_k)}
  \\
  &= A_k^{-1} - \frac{(A_k^{-1}\vec{y}_k - \vec{s}_k) \vec{s}_k^TA_k^{-1}}{\vec{s}_k^TA_k^{-1}\vec{y}_k} \\
  &= A_k^{-1} + \frac{(\vec{s}_k - A_k^{-1}\vec{y}_k)\vec{s}_k^T A_k^{-1}}{\vec{s}_k^T A_k^{-1}\vec{y}_k}
\end{align*}

\textbf{Exercise 9.18} \\
Let $Q \in M_n(\mathbb{R})$ satisfy $Q > 0$, and let $f$ be the quadratic function $f(\vec{x}) = \frac{1}{2}\vec{x}^TQ\vec{x} - \vec{b}^T\vec{x} + c$. Given a starting point $\vec{x}_0$ and $Q$-conjugate directions $\vec{d}_0, \vec{d}_1, ..., \vec{d}_{n-1}$ in $\mathbb{R}^n$, the optimal line search solution for $x_{k+1} = \vec{x}_k + \alpha_k \vec{d}_k$ (that is, the $\alpha$ which minimizes $\phi_k(\alpha) = f(\vec{x}_k+\alpha_k \vec{d}_k)$) is given by $\alpha_k = \frac{\vec{r}_k^T \vec{d}_k}{\vec{d}_k^T Q \vec{d}_k}$, where $\vec{r}_k = \vec{b} - Q\vec{x}_k$. \\
\textit{Proof:} \\
The optimal line search solution for $x_{k+1} = \vec{x}_k + \alpha_k \vec{d}_k$ is the $\alpha$ which minimizes $\phi_k(\alpha) = f(\vec{x}_k+\alpha_k \vec{d}_k)$. By the FONC, we have that
\begin{align*}
  &Df(\vec{x}_k+\alpha_k \vec{d}_k)\vec{d}_k = \vec{0}
  \\
  &\implies ((\vec{x}_k+\alpha_k \vec{d}_k)^TQ - \vec{b}^T)\vec{d}_k = \vec{0}
  \\
  &\implies \vec{x}_k^TQ\vec{d}_k + \alpha_k\vec{d}_k^T Q \vec{d}_k - \vec{b}^T\vec{d}_k = \vec{0}
  \\
  &\implies \alpha_k = \frac{(\vec{b}^T - \vec{x}_k^TQ)\vec{d}_k}{\vec{d}_k^T Q \vec{d}_k}
  \\
  &\implies \alpha_k = \frac{\vec{r}_k^T \vec{d}_k}{\vec{d}_k^T Q \vec{d}_k}
\end{align*}
where $\vec{r}_k = \vec{b} - Q\vec{x}_k$. \\

\end{document}
